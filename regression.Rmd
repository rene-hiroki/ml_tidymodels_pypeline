## Description

press "ctrl + shift + o (windows)" to show document outline
press "alt + o" to collapse all
press "alt + shift + o" to expand all

## Load Librarys

```{r}
library(tidyverse)
library(tidymodels)
library(naniar)     # vis_miss
library(janitor)    # rename colnames to under score
library(furrr)      # parallel
library(rlang)      # expression
library(earth)      # modeling mars
library(xgboost)    # modeling xgboost
library(lightgbm)   # modeling lgbm
library(Matrix)     # modeling lgbm
library(tictoc)     # time check
library(patchwork)  # plot
source("func.R")
```

## Read Data 

**depends on project**

```{r}
data <- diamonds
glimpse(diamonds)
```

## EDA

**depends on project**
**or separate the scripts**

```{r}
# vis_miss(data)
```



## Split the Data and CV Data

```{r}
set.seed(7777)
splits <- data %>% initial_split(prop = 0.8)

train <- training(splits)
test <- testing(splits)

data_cv <- train %>% vfold_cv(v = 5) 
  
```

## Recipes (Preprocessing)

```{r recipes list, eval=FALSE, include=FALSE}
# step_inverse() #逆変換
# step_log()
# step_sqrt()
# step_logit()
# step_invlogit()
# step_BoxCox()
# step_YeoJohnson() # boxcox の改良版
# step_relu() # NNでよく使う発火関数
# step_hyperbolic() # デフォルトsin
# step_meanimpute() # 平均で欠損補完
# step_modeimpute() # 最頻値で欠損補完
# step_rollimpute() # スライド窓で欠損補完、デフォルト中央値
# step_knnimpute() # k近傍法で欠損補完
# step_bagimpute() # 決定木のバギングで欠損補完
# step_rm() # 名前や方に基づいて変数消去
# step_corr() # 閾値より上の相関がある変数を片方消去
# step_zv() # 分散が０の変数消去
# step_nzv() # 分散が０に近い変数の消去
# step_lincomb() # 他と線形の関係にある変数を消去
# step_num2factor() # 数値をファクターに変換
# step_string2factor() # 文字列をファクター
# step_factor2string() # ファクターを文字列
# step_ordinalscore() # 順序カテゴリを数値に
# step_novel() # 初見のカテゴリに新しいファクター（主にテストデータ
# step_center() # データの中心からの距離を算出（中心化）
# step_scale() # 標準化
# remains other step ...
```


```{r}
# set target_var, sym means symbol
target_var <- sym("carat")

# set estimate type
mode_reg <- "regression"

# set formura
formula <- expr(formula(!!target_var ~ .))

# set recipes
rec <-
  data %>%
  recipe(formula = formula) %>%
  step_ordinalscore(cut, color, clarity)

rec_preped <- prep(rec)
rec_preped %>% juice()
```


## Modeling

### Prepare the Model

```{r model and engine list, eval=FALSE, include=FALSE}
# boost_tree()    # spark, xgboost
# decision_tree() # rpart, spark
# linear_reg()    # glmnet, keras, lm, spark, stan
# mars()
# mlp()           # keras, nnet
# nearest_neighbor()  # kknn
# null_model()    # parsnip
# rand_forest()   # randomForest, ranger, spark
# surv_reg()
# svm_poly()     # kernlab
# svm_rbf()      # kernlab

```


```{r}
# linear regression
model_lm <-
  linear_reg() %>% 
  set_engine("lm") %>%
  set_mode(mode_reg) 

# random forest
model_rf <-
  rand_forest() %>% 
  set_engine("ranger") %>%
  set_mode(mode_reg) 

# mars model
model_mars <-
  mars() %>% 
  set_engine("earth") %>% 
  set_mode(mode_reg)

#boosted trees
model_xgb <-
  boost_tree() %>%
  set_engine("xgboost") %>% 
  set_mode(mode_reg)

#neural nets
model_keras <-
  mlp() %>%
  set_engine("keras") %>% 
  set_mode(mode_reg)

model_list <-
  list(
    lm = model_lm,
    rf = model_rf,
    mars = model_mars,
    xgb = model_xgb,
    nnet = model_keras
  )
# model summary
model_list %>% map(translate)
```

### Soro model

#### XGBoost

```{r}
# fitting
fit_xgb <- fit(model_list$xgb, formula = formula(formula), data = juice(rec_preped))

# predict train
res_xgb <-
  train %>% 
  bake(rec_preped, new_data = .) %>% 
  predict(fit_xgb, new_data = .) %>% 
  bind_cols(train %>% select(target_var))

res_xgb
```


#### lightGBM

```{r}
# create lgb data set
lgb_train_data <-
  create_lgb_dataset(target_var = target_var,
                     rec_preped = rec_preped,
                     data = train)

# set lgb params
params <-
  list(objective = "regression",
       metric = "rmse")
```

```{r}
# fit
tic()
fit_lgb <-
  lgb.train(params = params,
            data = lgb_train_data$data)
toc()
# predict
res_lgb <-
  tibble(.pred = predict(fit_lgb, lgb_dataset$mat)) %>%
  bind_cols(train %>% select(target_var))

```

#### feature importance

```{r}
my_importance_plot(fit_xgb, res_xgb)
my_importance_plot(fit_lgb, res_lgb)

```

#### metric

```{r}
bind_rows(
  res_xgb %>% metrics(carat, .pred) %>% mutate(boost = "xgb"),
  res_lgb %>% metrics(carat, .pred) %>% mutate(boost = "lgb")
) %>%
  arrange(.metric) %>% 
  select(-.estimator)
```

**maybe return to EDA and preprocessing**

### Cross Validation

```{r}
# plan(multiprocess)
# select cv model
cv_model <- c("lm", "xgb")
res_cv <- fit_cv(model_list[cv_model])
res_cv
```

### CV Result

```{r}
res_cv$fits %>% map(collect_metrics) %>% map2(cv_model, ~mutate(.x, method = .y)) %>% bind_rows()
res_cv$fits %>% map(collect_predictions)
```

## Tuning 

### Prepare the Models (tuning)

```{r}
# linear regression (lasso)
tune_lm <-
  linear_reg(penalty = tune(),
             mixture = 1) %>%
  set_engine("glmnet") %>%
  set_mode(mode_reg)

# Hyperparameter grid
grid_lm <-
  tune_lm %>%
  parameters() %>%
  grid_max_entropy(size = 5)

# random forest
tune_rf <- model_rf %>%
  update(mtry = tune(),
         trees = tune())

grid_rf <-
  tune_rf %>%
  parameters() %>%
  finalize(select(data, -target_var)) %>%
  grid_max_entropy(size = 5)

# mars model
tune_mars <- model_mars %>%
  update(
    num_terms = tune(),
    prod_degree = 2,
    prune_method = tune()
  )

grid_mars <-
  tune_mars %>%
  parameters() %>%
  grid_max_entropy(size = 5)

#boosted trees
tune_xgb <- model_xgb %>%
  update(
    mtry = tune(),
    tree = tune(),
    min_n = tune(),
    learn_rate = tune(),
    tree_depth = tune()
  )

grid_xgb <- tune_xgb %>%
  parameters() %>%
  finalize(select(data, -target_var)) %>%
  grid_max_entropy(size = 5)

#neural nets
tune_keras <- model_keras %>%
  update(hidden_units = tune(),
         penalty = tune(),
         activation = "relu")

grid_keras <-
  tune_keras %>%
  parameters() %>%
  grid_max_entropy(size = 5)

```


### Create Model and Grid List  (tuning)

```{r}
model_list_tuning <-
  list(lm = tune_lm, rf = tune_rf, mars = tune_mars, xgb = tune_xgb)
grid_list <-
  list(lm = grid_lm, rf = grid_rf, mars = grid_mars, xgb = grid_xgb)
model_list_tuning
grid_list
```

### Modeling by Tuning CV

```{r}
plan(multiprocess)

fits_tuning <- fit_cv_tuning(model_list_tuning, grid_list)
fits_tuning
```


```{r}
fits_no_tuning$fit %>% map(collect_metrics)
fits_no_tuning$fit %>% map(collect_predictions)
```

```{r}
fits_tuning$fit %>% map(collect_metrics) %>%  map(arrange,.metric, mean)
```

```{r}
fits_tuning$fit %>% map(collect_predictions) 
```

```{r}
fits_tuning$fit %>% map(show_best, metric = "rmse", maximize = FALSE, n = 3)
```


## Refit by Best Params

### Update Models to Best Params

```{r}
best_param_list <-
  fits_tuning$fit %>% 
    map(show_best, metric = "rmse", maximize = FALSE, n = 1) %>% 
    map(select, -c(".metric", ".estimator", "mean", "n", "std_err"))

best_tune_lm <- tune_lm %>% update(best_param_list$lm)
best_tune_rf <- tune_rf %>% update(best_param_list$rf)
best_tune_mars <- tune_mars %>% update(best_param_list$mars)
best_tune_xgb <- tune_xgb %>% update(best_param_list$xgb)

```

### Create Model List  (best params)

```{r}
model_list_best_params <-
  list(lm = best_tune_lm, rf = best_tune_rf, mars = best_tune_mars, xgb = best_tune_xgb)
```

### Final Modeling by Best Params 

```{r}

fit_final_model <- function(model, rec_preped){
  fits <- fit(model, formula = formula(formula), data = juice(rec_preped))
  return(fits)
}

plan(multiprocess)
fits_final <-
  model_list_best_params %>% 
    future_map(
      ~fit_final_model(
        .x,
        rec_preped = rec_preped
        )
      )
```

## Predict Test

```{r}
baked_test <- 
  test %>% 
  bake(rec_preped, new_data = .)

preds_list <- 
  fits_final[2:4] %>%
  map(predict, baked_test %>% select(-carat)) 

preds_d <- preds_list %>% bind_cols() 

colnames(preds_d) <- paste0("pred_", names(preds_list))

baked_test <- baked_test %>% bind_cols(preds_d) 
baked_test
```

### plot
```{r}
colnames(preds_d) %>% map(my_test_confirm_plot)
```

## Final test (submit)


```{r}
saveRDS(fits_tuning, "fits_tuning.rds")
saveRDS(fits_best_params, "fits_best_params.rds")
```


